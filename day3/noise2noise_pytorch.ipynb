{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/constantinpape/training-deep-learning-models-for-vison/blob/master/day3/noise2noise_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise2Noise U-Net\n",
    "\n",
    "In this notebook, we will implement \"Noise2Noise: Learning Image Restoration without Clean Data\". See the [original paper](https://arxiv.org/abs/1803.04189) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing\n",
    "For this excercise will use the [VDSR dataset](https://cv.snu.ac.kr/research/VDSR) containing 'clean' (no noise added) natural images.\n",
    "\n",
    "Let's start with downloading and unzipping the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://cv.snu.ac.kr/research/VDSR/train_data.zip\n",
    "!wget https://cv.snu.ac.kr/research/VDSR/test_data.zip\n",
    "!mkdir -p vdsr_train\n",
    "!mkdir -p vdsr_test\n",
    "!unzip -qq train_data.zip -d vdsr_train && rm train_data.zip\n",
    "!unzip -qq test_data.zip -d vdsr_test && rm test_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in previous excerices we're going to create our custom `Dataset` class for loading all the images from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the below implementation the 'noise_transform' should be a Callable which takes an image and returns\n",
    "# a tuple of two images\n",
    "\n",
    "class VdsrDataset(Dataset):\n",
    "    def __init__(self, root_dir, noise_transform, crop_size=256):\n",
    "        image_suffixes = (\".jpeg\", \".jpg\", \".png\", \".bmp\")\n",
    "        self.image_paths = [p for p in Path(root_dir).glob(\"**/*\") if p.suffix.lower() in image_suffixes]\n",
    "        assert noise_transform is not None\n",
    "        # transforms the image according to the noise model\n",
    "        self.noise_transform = noise_transform\n",
    "        #  standard transformations to apply to the input images\n",
    "        self.inp_transforms = transforms.Compose([\n",
    "            # randomly crop the image, paddig if necessary\n",
    "            transforms.RandomCrop(crop_size, pad_if_needed=True, padding_mode='reflect'),\n",
    "            # converts numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    # get the total number of samples\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    # fetch the training sample given its index\n",
    "    def __getitem__(self, idx):\n",
    "        # read image from disk\n",
    "        img = imageio.imread(self.image_paths[idx])\n",
    "        # convert signle-channel images\n",
    "        if img.ndim == 2:\n",
    "            img = np.stack([img, img, img], axis=2)\n",
    "        elif img.shape[2] == 1:\n",
    "            img = np.concatenate([img, img, img], axis=2)\n",
    "        # convet to PIL image\n",
    "        img = Image.fromarray(img)\n",
    "        # apply standard augmentations\n",
    "        img = self.inp_transforms(img)\n",
    "        # convert [0, 1] to [-0.5, 0.5]\n",
    "        img = img - 0.5\n",
    "        # apply the noise model and return a source and target image\n",
    "        return self.noise_transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the training dataset and show some of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = 'vdsr_train'\n",
    "# dummy transform, which just returns the input image twice\n",
    "t = lambda x: (x, x)\n",
    "train_data = VdsrDataset(TRAIN_DATA_PATH, noise_transform=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_to_uint8(arr):\n",
    "    \"\"\"\n",
    "    Converts a given torch tensor assumed to be [-0.5, 0.5]-normalized into a uint8 tensor\n",
    "    \"\"\"\n",
    "    return torch.clamp((arr + 0.5) * 255.0 + 0.5, 0, 255).type(torch.uint8)\n",
    "\n",
    "def show_random_dataset_image(dataset):\n",
    "    idx = np.random.randint(0, len(dataset))    # take a random sample\n",
    "    source, target = dataset[idx]                    # get the source and target image\n",
    "    # convert (C,H,W) to (H,W,C)\n",
    "    source = np.transpose(source, (1, 2, 0)) \n",
    "    target = np.transpose(target, (1, 2, 0))\n",
    "    # covert to uint8\n",
    "    source = clip_to_uint8(source)\n",
    "    target = clip_to_uint8(target)\n",
    "    \n",
    "    f, axarr = plt.subplots(1, 2)               # make two plots on one figure\n",
    "    axarr[0].imshow(source)                     # show the image\n",
    "    axarr[1].imshow(target)                    # show the masks\n",
    "    _ = [ax.axis('off') for ax in axarr]        # remove the axes\n",
    "    print(f'Image size is {source.shape}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_dataset_image(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's create our noise model. We will study the effect of corrupted targets using\n",
    "synthetic additive Gaussian noise.\n",
    "\n",
    "We randomize the noise standard deviation σ ∈ [0, 50] separately for\n",
    "each training example to make it more difficult for the network, i.e., the network has to estimate the\n",
    "magnitude of noise while removing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentGaussian:\n",
    "    def __init__(self, train_stddev_rng_range):\n",
    "        assert len(train_stddev_rng_range) == 2\n",
    "        self.minval, self.maxval = train_stddev_rng_range\n",
    "        self.minval = self.minval / 255\n",
    "        self.maxval = self.maxval / 255\n",
    "\n",
    "    def __call__(self, x):\n",
    "        rng_stddev = (self.maxval - self.minval) * torch.rand(1) + self.minval\n",
    "        return x + torch.randn(x.size()) * rng_stddev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our noise augmentor, which randomize the noise standard deviation for source and target image separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additive_gaussian_noise_train = AugmentGaussian((0, 50))\n",
    "\n",
    "# this time our 'noise_transform' Callable returns two images agumented with Gaussian noise with different standard deviation.\n",
    "TRAIN_NOISE_TRANSFORM = lambda x: (additive_gaussian_noise_train(x), additive_gaussian_noise_train(x))\n",
    "\n",
    "# create the training Dataset with our noise transformer\n",
    "TRAIN_DATA_PATH = 'vdsr_train'\n",
    "train_data = VdsrDataset(TRAIN_DATA_PATH, noise_transform=TRAIN_NOISE_TRANSFORM)\n",
    "# create the DataLoader with batch size of 4\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_dataset_image(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create validation loader. This time we use a fixed standard deviation of 25 for the additive Gaussian noise. Notice that now the 2nd (i.e. target) image returned from the loader is not augmented with noise. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additive_gaussian_noise_val = AugmentGaussian((25, 25))\n",
    "# apply noise augmentation only to the input, leaving the target clean\n",
    "VAL_NOISE_TRANSFORM = lambda x: (additive_gaussian_noise_val(x), x)\n",
    "\n",
    "# create validation Dataset\n",
    "VAL_DATA_PATH = 'vdsr_test'\n",
    "val_data = VdsrDataset(VAL_DATA_PATH, noise_transform=VAL_NOISE_TRANSFORM)\n",
    "# create DataLoader with a batch size of 4\n",
    "val_loader = DataLoader(val_data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_dataset_image(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net architecure \n",
    "\n",
    "We're going to use the same architectue as described in Appendix 1 of  J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Karras, M. Aittala, T. Aila, \"Noise2Noise: Learning Image Restoration without Clean Data\". Can you spot the main differences? What activation function has been used this time?\n",
    "\n",
    "Notice that denoising is a regression problem, so this time we're not normalizing the output of the last convolution with a Sigmoid function and take the linear activation directly instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\" UNet implementation\n",
    "    Arguments:\n",
    "      in_channels: number of input channels\n",
    "      out_channels: number of output channels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convolutional block for single layer of the decoder / encoder\n",
    "    # we apply to 2d convolutions with leaky ReLU activation\n",
    "    def _conv_block(self, in_channels, out_channels, block_num):\n",
    "        conv_blocks = []\n",
    "        for i in range(block_num):\n",
    "            if i == 0:\n",
    "                in_ch = in_channels\n",
    "            else:\n",
    "                in_ch = out_channels\n",
    "            # add convolutional layer\n",
    "            conv_blocks.append(nn.Conv2d(in_ch, out_channels, kernel_size=3, padding=1))\n",
    "            # add batchnorm for better training stability\n",
    "            conv_blocks.append(nn.BatchNorm2d(out_channels))\n",
    "            # add activation function\n",
    "            conv_blocks.append(nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "            \n",
    "        return nn.Sequential(*conv_blocks)       \n",
    "\n",
    "\n",
    "    # upsampling via nearest-neighbor interpolation\n",
    "    def _upsample(self, x, size):\n",
    "        return F.interpolate(x, size=size, mode='nearest')\n",
    "    \n",
    "    # we do use a final Sigmoid activation this time, since we're dealing with a regression problem\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the depth (= number of encoder / decoder levels) is\n",
    "        # hard-coded to 5\n",
    "        self.depth = 5\n",
    "        \n",
    "        # all lists of conv layers (or other nn.Modules with parameters) must be wraped\n",
    "        # itnto a nn.ModuleList\n",
    "        \n",
    "        # modules of the encoder path\n",
    "        self.encoder = nn.ModuleList([self._conv_block(in_channels, 48, 2),\n",
    "                                      self._conv_block(48, 48, 1),\n",
    "                                      self._conv_block(48, 48, 1),\n",
    "                                      self._conv_block(48, 48, 1),\n",
    "                                      self._conv_block(48, 48, 1)])\n",
    "        # the base convolution block\n",
    "        self.base = self._conv_block(48, 48, 1)\n",
    "        # modules of the decoder path\n",
    "        self.decoder = nn.ModuleList([self._conv_block(96, 96, 2),\n",
    "                                      self._conv_block(144, 96, 2),\n",
    "                                      self._conv_block(144, 96, 2),\n",
    "                                      self._conv_block(144, 96, 2),\n",
    "                                      self._conv_block(144, 64, 2)])\n",
    "        \n",
    "        # the pooling layers; we use 2x2 MaxPooling\n",
    "        self.poolers = nn.ModuleList([nn.MaxPool2d(2) for _ in range(self.depth)])\n",
    "        \n",
    "        # output conv with linear activation\n",
    "        self.out_conv = nn.Conv2d(64, out_channels, 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        # apply encoder path\n",
    "        encoder_out = []\n",
    "        for level in range(self.depth):\n",
    "            x = self.encoder[level](x)\n",
    "            encoder_out.append(x)\n",
    "            x = self.poolers[level](x)\n",
    "\n",
    "        # apply base\n",
    "        x = self.base(x)\n",
    "        \n",
    "        # apply decoder path\n",
    "        encoder_out = encoder_out[::-1]\n",
    "        for level in range(self.depth):\n",
    "            # get the spatial dimension of the corresponding encoder features\n",
    "            size = encoder_out[level].size()[2:]\n",
    "            x = self._upsample(x, size)\n",
    "            x = self.decoder[level](torch.cat((x, encoder_out[level]), dim=1))\n",
    "        \n",
    "        # apply output conv\n",
    "        x = self.out_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the network architecture implemented, let's make a single forward pass with a random image in order to see that it's working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = UNet(in_channels=3, out_channels=3)\n",
    "\n",
    "idx = np.random.randint(0, len(train_data))\n",
    "img = train_data[idx][0]\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "output = m(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and evaluation metrics\n",
    "\n",
    "Since we're using the additive Gaussian noise, which has a zero mean, we will use an `L_2` (`MSE`) loss.\n",
    "\n",
    "[Peak signal-to-noise ratio](https://en.wikipedia.org/wiki/Peak_signal-to-noi) will be used as our evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_CRITERION = nn.MSELoss()\n",
    "\n",
    "class PSNR:\n",
    "    def __call__(self, image_true, image_test):\n",
    "        image_true = clip_to_uint8(image_true)\n",
    "        image_test = clip_to_uint8(image_test)\n",
    "        image_true = image_true.detach().cpu().numpy()\n",
    "        image_test = image_test.detach().cpu().numpy()\n",
    "        return peak_signal_noise_ratio(image_true, image_test)\n",
    "    \n",
    "EVAL_METRIC = PSNR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply training for one epoch\n",
    "def train(model, loader, optimizer, loss_function,\n",
    "          epoch, log_interval=100, tb_logger=None):\n",
    "\n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "    # iterate over the batches of this epoch\n",
    "    for batch_id, (x, y) in enumerate(loader):\n",
    "        # move input and target to the active device (either cpu or gpu)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # zero the gradients for this iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # apply model, calculate loss and run backwards pass\n",
    "        prediction = model(x)\n",
    "        loss = LOSS_CRITERION(prediction, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # log to console\n",
    "        if batch_id % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                  epoch, batch_id * len(x),\n",
    "                  len(loader.dataset),\n",
    "                  100. * batch_id / len(loader), loss.item()))\n",
    "\n",
    "            # log to tensorboard\n",
    "            if tb_logger is not None:\n",
    "                step = epoch * len(loader) + batch_id\n",
    "                tb_logger.add_scalar(tag='train_loss', scalar_value=loss.item(), global_step=step)\n",
    "                \n",
    "                x, y, prediction = clip_to_uint8(x), clip_to_uint8(y), clip_to_uint8(prediction)\n",
    "                tb_logger.add_images(tag='input', img_tensor=x.to('cpu'), global_step=step)\n",
    "                tb_logger.add_images(tag='target', img_tensor=y.to('cpu'), global_step=step)\n",
    "                tb_logger.add_images(tag='prediction', img_tensor=prediction.to('cpu').detach(), global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run validation after training epoch\n",
    "def validate(model, loader, loss_function, metric, step=None, tb_logger=None):\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "    # running loss and metric values\n",
    "    val_loss = 0\n",
    "    val_metric = 0\n",
    "    \n",
    "    # disable gradients during validation\n",
    "    with torch.no_grad():\n",
    "        # iterate over validation loader and update loss and metric values\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            prediction = model(x)\n",
    "            loss = LOSS_CRITERION(prediction, y)\n",
    "            eval_score = EVAL_METRIC(y, prediction)\n",
    "            \n",
    "            val_loss += loss\n",
    "            val_metric += eval_score\n",
    "    \n",
    "    # normalize loss and metric\n",
    "    val_loss /= len(loader)\n",
    "    val_metric /= len(loader)\n",
    "    \n",
    "    if tb_logger is not None:\n",
    "        assert step is not None, \"Need to know the current step to log validation results\"\n",
    "        tb_logger.add_scalar(tag='val_loss', scalar_value=val_loss, global_step=step)\n",
    "        tb_logger.add_scalar(tag='val_metric', scalar_value=val_metric, global_step=step)\n",
    "        # we always log the last validation images\n",
    "        x, y, prediction = clip_to_uint8(x), clip_to_uint8(y), clip_to_uint8(prediction)\n",
    "        tb_logger.add_images(tag='val_input', img_tensor=x.to('cpu'), global_step=step)\n",
    "        tb_logger.add_images(tag='val_target', img_tensor=y.to('cpu'), global_step=step)\n",
    "        tb_logger.add_images(tag='val_prediction', img_tensor=prediction.to('cpu'), global_step=step)\n",
    "        \n",
    "    print('\\nValidate: Average loss: {:.4f}, Average Metric: {:.4f}\\n'.format(val_loss, val_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we have  a gpu\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a tensorboard writer\n",
    "logger = SummaryWriter('runs/noise2noise')\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to create the optimizer\n",
    "def create_optimizer(learning_rate, model):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99))\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(in_channels=3, out_channels=3)\n",
    "\n",
    "# move the model to GPU\n",
    "net = net.to(device)\n",
    "\n",
    "# use adam optimizer\n",
    "optimizer = create_optimizer(learning_rate=0.001, model=net)\n",
    "\n",
    "n_epochs = 40\n",
    "for epoch in range(n_epochs):\n",
    "    # train\n",
    "    train(net, train_loader, optimizer, LOSS_CRITERION, epoch, log_interval=25, tb_logger=logger)\n",
    "    step = epoch * len(train_loader.dataset)\n",
    "    # validate\n",
    "    validate(net, val_loader, LOSS_CRITERION, EVAL_METRIC, step=step, tb_logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Train a separete denoising model using clean target and compare the PSNR scores with those obtained with noise2noise model. Compare results of the two models visually in tensorboard.\n",
    "**Hint** the only change that needs to be done in the loader is changing the noise transformer to return a clean image during training\n",
    "```\n",
    "TRAIN_NOISE_TRANSFORM = lambda x: (additive_gaussian_noise_train(x), x)\n",
    "```\n",
    "2. Train noise2noise with different noise model, e.g. Poisson noise with varying lambda. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adl-course] *",
   "language": "python",
   "name": "conda-env-adl-course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
